# -*- coding: utf-8 -*-
"""LSTM by hanna.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1B1xyn8eZf3iSK_pv5ZciU5IiJ7iY_8xH
"""

# 개인저장소에 있는 파일(inputdata)을 불러오기
from google.colab import files
myfile = files.upload()

#모듈설치
import tensorflow as tf
import numpy as np
import pandas as pd
import matplotlib
import matplotlib.pyplot as plt
from keras.models import Sequential
from keras.layers import LSTM, Dense, Dropout
from sklearn.model_selection import train_test_split



# input data 데이터프레임(표 형식)으로 변수에 저장해주기
X_data = pd.read_csv("816_pressure.csv")


X_data.info()

X_data.plot()

# 데이터가 3.3~3.6 범위에 모여있으니 min-max scaler를 사용해 0과 1사이의 값으로 만들어줌
from sklearn.preprocessing import MinMaxScaler

#스케일러 생성
scaler = MinMaxScaler()

# Water_Pressure 열에 Min-Max 스케일링 적용
scale_coles = ['Water_Pressure']
X_scaled = scaler.fit_transform(X_data[scale_coles])

# 스케일링된 데이터 셋을 데이터프레임(표 형식)으로 다시 변수에 저장
X_scaled = pd.DataFrame(X_scaled, columns = scale_coles)

# 저장이 잘 됐는지 변수 보기
print(X_scaled.shape)

plt.figure(figsize=(7,4))
plt.title("Pressure") #그래프제목
plt.ylabel('Pressure') #y축 범례
plt.xlabel('Time') #x축 범례
plt.plot(X_scaled['Water_Pressure'], label ='Pressure', color='b') # 데이터의 waterpressure 열로 파란색(color = 'b') 그래프 만들기
plt.show()

# 시퀀스 길이 (몇일을 바탕으로 예측할건지) 생성
window_size = 5

# 시간 컬럼이 아닌 값이 적혀져있는 컬럼 따로 호출, 시계열데이터 특성상 feature label 둘이 똑같은 컬럼 사용
scale_coles = ['Water_Pressure']
X = pd.DataFrame(X_scaled, columns = scale_coles)
Y = pd.DataFrame(X_scaled, columns = scale_coles)

# features 는 시계열 데이터에서의 X 입력데이터 labels 는  시계열 데이터에서 Y 목표데이터. window_size - sequance 길이.

# 시퀀셜 데이터로 만드는 함수 (빈리스트에 데이터 추가하는 형태)

def make_sequence_dataset(X, Y, window_size):
    feature_list = []
    label_list = []
    for i in range(len(X) - window_size):
        x = X.iloc[i:(i + window_size)]
        y = Y.iloc[i + window_size]
        feature_list.append(x)
        label_list.append(y)
    return np.array(feature_list), np.array(label_list)

# 함수 호출
feature, label = make_sequence_dataset(X, Y, window_size)

# 입력데이터와 목표데이터의 형태 확인
print(feature.shape)
print(label.shape)

# 학습데이터, 검증데이터, 시험데이터 분리. 검증은 한 에포크가 끝날때마다 오류를 측정할때 쓰는데이터이고, 테스트는 모델 학습이 끝나고 최종 오류를 산정하는 데이터.
from sklearn.model_selection import train_test_split

X_train, X_temp, y_train, y_temp = train_test_split(feature, label, test_size=0.2, random_state=42) # train data : val data : test data = 8:1:1
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42) # val data와 test data를 1:1로 나누는 과정



#데이터 몇 개로 분류됐나 출력
print(X_train.shape, X_val.shape, X_test.shape)
print(y_train.shape, y_val.shape, y_test.shape)

# LSTM 모델 구축
# 중요! lstm(**) 이 숫자를 높이면 lstm의 레이어가 높아져 모델의 복잡도가 높아져서 정확도가 상승, 오버피팅 문제 발생가능! 일반적으로 2의배수로 조정
# activation은 활성화함수를 나타내는 것으로 lstm 에서는 주로 출력값이 -1에서1사이인 tanh함수를 사용한다

Model = Sequential()
Model.add(LSTM(50, activation='tanh', input_shape=(window_size, 1)))
Model.add(Dense(1, activation= 'linear'))
Model.summary()

# 모델 컴파일(옵티마이저, 로스함수, 모델평가지표 정하기) 후 학습
# 회귀 문제의 경우 주로 adam, mean squared error 사용
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.optimizers import Adam

Model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error', metrics=['mae'])


# 모델이 patience 에폭 동안 검증손실이 줄어들지않으면 학습 조기종료! (오버피팅 줄이기)
early_stopping = EarlyStopping(monitor='val_loss', patience=30, restore_best_weights=True)



# 훈련
history = Model.fit(X_train, y_train, epochs=300, batch_size=32, validation_data=(X_val, y_val), callbacks=[early_stopping])

import matplotlib.pyplot as plt

# 훈련 손실과 검증 손실
plt.figure(figsize=(8, 4))
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Model Loss')
plt.ylabel('Loss')
plt.xlabel('Epochs')
plt.legend()
plt.show()

# 평균 절대 오차(MAE)
plt.figure(figsize=(8, 4))
plt.plot(history.history['mae'], label='Train MAE')
plt.plot(history.history['val_mae'], label='Validation MAE')
plt.title('Model MAE')
plt.ylabel('MAE')
plt.xlabel('Epochs')
plt.legend()
plt.show()

from sklearn.metrics import mean_squared_error

# X_test를 사용한 예측 수행
predicted = Model.predict(X_test)

MSE = mean_squared_error(y_test,predicted)
RMSE = np.sqrt(MSE)
print('RMSE :', RMSE)

from sklearn.metrics import r2_score
R2= r2_score(y_test, predicted)
print('R2_score :',R2)

N = 1  # 가정: 마지막 5개 데이터 포인트를 사용
last_points = X_train[-N:]

# LSTM 모델에 맞게 형태 조정
# 예: 하나의 특성만 있는 경우
last_points_array = last_points.reshape(1, 5, 1)

next_value = Model.predict(last_points_array)

# 예측 결과 출력
print("Next predicted value:", next_value[0][0])

# 스케일링된 데이터 복구하여 실제값과 비교
scaler = MinMaxScaler()
scaler.fit(X_data[scale_coles])
# 스케일링된 데이터 복구
y_test_original = scaler.inverse_transform(y_test)
predicted_original = scaler.inverse_transform(predicted)

# 그래프 그리기
plt.figure(figsize=(12, 6))
plt.plot(y_test_original, label='True')
plt.plot(predicted_original, label='Pred')
plt.title('Actual vs Predicted Values')
plt.xlabel('Time Steps')
plt.ylabel('Original Values')
plt.legend()
plt.show()



